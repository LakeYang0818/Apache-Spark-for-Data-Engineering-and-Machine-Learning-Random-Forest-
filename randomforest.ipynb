{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "previous-irish",
   "metadata": {
    "papermill": {
     "duration": 0.005346,
     "end_time": "2021-03-17T17:21:55.111968",
     "exception": false,
     "start_time": "2021-03-17T17:21:55.106622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# RandomForest Classification\n",
    "\n",
    "__build an end to end pipeline that reads in data in parquet format, converts it to CSV and loads it into a dataframe, trains a model and perform hyperparameter tuning. For this submission, you may use code and snippets from all the resources mentioned above including the component library. Create a notebook that does the following:__\n",
    "\n",
    "- Read in the parquet file you created as part of Task 3.\n",
    "- Convert the parquet file to CSV format.\n",
    "- Load the CSV file into a dataframe\n",
    "- Create a 80-20 training and test split with seed=1.\n",
    "- Train a Random Forest model with different hyperparameters listed below and report the best performing hyperparameter combinations.\n",
    "\n",
    "### Hyper parameters:\n",
    "  - number of trees : {10, 20}\n",
    "  - maximum depth : {5, 7} \n",
    "  - use random seed = 1 wherever needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "palestinian-toilet",
   "metadata": {
    "papermill": {
     "duration": 2.451598,
     "end_time": "2021-03-17T17:21:57.568763",
     "exception": false,
     "start_time": "2021-03-17T17:21:55.117165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "Starting installation...\n",
      "Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export version=`python --version |awk '{print $2}' |awk -F\".\" '{print $1$2}'`\n",
    "\n",
    "echo $version\n",
    "\n",
    "if [ $version == '36' ] || [ $version == '37' ]; then\n",
    "    echo 'Starting installation...'\n",
    "    pip3 install pyspark==2.4.8 wget==3.2 pyspark2pmml==0.5.1 > install.log 2> install.log\n",
    "    if [ $? == 0 ]; then\n",
    "        echo 'Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)'\n",
    "    else\n",
    "        echo 'Installation failed, please check log:'\n",
    "        cat install.log\n",
    "    fi\n",
    "elif [ $version == '38' ] || [ $version == '39' ]; then\n",
    "    pip3 install pyspark==3.1.2 wget==3.2 pyspark2pmml==0.5.1 > install.log 2> install.log\n",
    "    if [ $? == 0 ]; then\n",
    "        echo 'Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)'\n",
    "    else\n",
    "        echo 'Installation failed, please check log:'\n",
    "        cat install.log\n",
    "    fi\n",
    "else\n",
    "    echo 'Currently only python 3.6, 3.7 , 3.8 and 3.9 are supported, in case you need a different version please open an issue at https://github.com/IBM/claimed/issues'\n",
    "    exit -1\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "soviet-tobacco",
   "metadata": {
    "papermill": {
     "duration": 0.009853,
     "end_time": "2021-03-17T17:21:57.584134",
     "exception": false,
     "start_time": "2021-03-17T17:21:57.574281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @param data_dir temporal data storage for local execution\n",
    "# @param data_csv csv path and file name (default: data.csv)\n",
    "# @param data_parquet path and parquet file name (default: data.parquet)\n",
    "# @param master url of master (default: local mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nearby-transmission",
   "metadata": {
    "papermill": {
     "duration": 0.132622,
     "end_time": "2021-03-17T17:21:57.721932",
     "exception": false,
     "start_time": "2021-03-17T17:21:57.589310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import fnmatch\n",
    "from pathlib import Path\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark2pmml import PMMLBuilder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "import site\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "productive-congress",
   "metadata": {
    "papermill": {
     "duration": 0.010157,
     "end_time": "2021-03-17T17:21:57.737545",
     "exception": false,
     "start_time": "2021-03-17T17:21:57.727388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_csv = os.environ.get('data_csv', 'data.csv')\n",
    "data_parquet = os.environ.get('data_parquet', 'data.parquet')\n",
    "master = os.environ.get('master', \"local[*]\")\n",
    "data_dir = os.environ.get('data_dir', '../../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "associate-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet = 'data.parquet'\n",
    "data_csv = 'data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "indie-classics",
   "metadata": {
    "papermill": {
     "duration": 0.028188,
     "end_time": "2021-03-17T17:21:57.786591",
     "exception": false,
     "start_time": "2021-03-17T17:21:57.758403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/04 10:44:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/04 10:45:00 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/04 10:45:00 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(master))\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72045811-9859-4099-a762-e47f6b9b181b",
   "metadata": {},
   "source": [
    "### Step 1. Read in the parquet file you created as part of Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "finnish-shaft",
   "metadata": {
    "papermill": {
     "duration": 0.010092,
     "end_time": "2021-03-17T17:21:57.802836",
     "exception": false,
     "start_time": "2021-03-17T17:21:57.792744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(data_dir + data_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "competitive-whole",
   "metadata": {
    "papermill": {
     "duration": 0.008981,
     "end_time": "2021-03-17T17:21:57.816796",
     "exception": false,
     "start_time": "2021-03-17T17:21:57.807815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../../data/data.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(data_dir + data_csv):\n",
    "    shutil.rmtree(data_dir + data_csv)\n",
    "df.coalesce(1).write.option(\"header\", \"true\").csv(data_dir + data_csv)\n",
    "file = glob.glob(data_dir + data_csv + '/part-*')\n",
    "shutil.move(file[0], data_dir + data_csv + '.tmp')\n",
    "shutil.rmtree(data_dir + data_csv)\n",
    "shutil.move(data_dir + data_csv + '.tmp', data_dir + data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "788c86af-1592-465f-87d5-489b8afd8318",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = list(\n",
    "    map(lambda s: re.sub('$', '\"', s),\n",
    "        map(\n",
    "            lambda s: s.replace('=', '=\"'),\n",
    "            filter(\n",
    "                lambda s: s.find('=') > -1 and bool(re.match(r'[A-Za-z0-9_]*=[.\\/A-Za-z0-9]*', s)),\n",
    "                sys.argv\n",
    "            )\n",
    "    )))\n",
    "\n",
    "for parameter in parameters:\n",
    "    logging.warning('Parameter: ' + parameter)\n",
    "    exec(parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1665671f-a46e-409d-a44e-a5fa1ba0fe3d",
   "metadata": {},
   "source": [
    "### Step 2. Convert the parquet file to CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3914ebfb-b538-4fdc-a701-cde983fa7af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(data_dir + data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7134b323-7ef0-4b9b-877e-7450de1780dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:============================================>              (6 + 2) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----+\n",
      "|_c0|_c1|_c2|                 _c3|  _c4|\n",
      "+---+---+---+--------------------+-----+\n",
      "|  x|  y|  z|              source|class|\n",
      "| 13| 45| 36|Accelerometer-201...| Walk|\n",
      "| 13| 45| 35|Accelerometer-201...| Walk|\n",
      "| 10| 46| 36|Accelerometer-201...| Walk|\n",
      "| 11| 44| 36|Accelerometer-201...| Walk|\n",
      "|  7| 45| 34|Accelerometer-201...| Walk|\n",
      "|  7| 46| 36|Accelerometer-201...| Walk|\n",
      "| 11| 42| 34|Accelerometer-201...| Walk|\n",
      "| 10| 41| 34|Accelerometer-201...| Walk|\n",
      "| 10| 41| 35|Accelerometer-201...| Walk|\n",
      "|  8| 39| 33|Accelerometer-201...| Walk|\n",
      "|  9| 39| 33|Accelerometer-201...| Walk|\n",
      "| 11| 38| 33|Accelerometer-201...| Walk|\n",
      "| 11| 38| 33|Accelerometer-201...| Walk|\n",
      "| 12| 39| 33|Accelerometer-201...| Walk|\n",
      "| 12| 40| 31|Accelerometer-201...| Walk|\n",
      "| 14| 40| 32|Accelerometer-201...| Walk|\n",
      "| 16| 38| 32|Accelerometer-201...| Walk|\n",
      "| 16| 40| 32|Accelerometer-201...| Walk|\n",
      "| 17| 38| 32|Accelerometer-201...| Walk|\n",
      "+---+---+---+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = df1.repartition(1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "919f8996-8738-43dc-aa35-36c89f293ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----+\n",
      "|  x|  y|  z|              source|class|\n",
      "+---+---+---+--------------------+-----+\n",
      "| 13| 45| 36|Accelerometer-201...| Walk|\n",
      "| 13| 45| 35|Accelerometer-201...| Walk|\n",
      "| 10| 46| 36|Accelerometer-201...| Walk|\n",
      "| 11| 44| 36|Accelerometer-201...| Walk|\n",
      "|  7| 45| 34|Accelerometer-201...| Walk|\n",
      "|  7| 46| 36|Accelerometer-201...| Walk|\n",
      "| 11| 42| 34|Accelerometer-201...| Walk|\n",
      "| 10| 41| 34|Accelerometer-201...| Walk|\n",
      "| 10| 41| 35|Accelerometer-201...| Walk|\n",
      "|  8| 39| 33|Accelerometer-201...| Walk|\n",
      "|  9| 39| 33|Accelerometer-201...| Walk|\n",
      "| 11| 38| 33|Accelerometer-201...| Walk|\n",
      "| 11| 38| 33|Accelerometer-201...| Walk|\n",
      "| 12| 39| 33|Accelerometer-201...| Walk|\n",
      "| 12| 40| 31|Accelerometer-201...| Walk|\n",
      "| 14| 40| 32|Accelerometer-201...| Walk|\n",
      "| 16| 38| 32|Accelerometer-201...| Walk|\n",
      "| 16| 40| 32|Accelerometer-201...| Walk|\n",
      "| 17| 38| 32|Accelerometer-201...| Walk|\n",
      "| 16| 38| 34|Accelerometer-201...| Walk|\n",
      "+---+---+---+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.option('header', 'true').csv(data_dir + data_csv)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0005710-03a6-45a6-b5b8-7d360db70db5",
   "metadata": {},
   "source": [
    "### Step 3. Load the CSV file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8ac59e9-a080-460d-8378-b5caf3976213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register a corresponding query table\n",
    "df1.createOrReplaceTempView('df1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ca37ddc-76cf-4087-876a-451ce9232147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "df1 = df1.withColumn(\"x\", df1.x.cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"y\", df1.y.cast(DoubleType()))\n",
    "df1 = df1.withColumn(\"z\", df1.z.cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d92bf76-dd05-4aea-a5f6-8efa3adb00f9",
   "metadata": {},
   "source": [
    "### Step 4. Create a 80-20 training and test split with seed=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "483a3ef4-e2d5-4218-85f1-bc7979b2ca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80-20 training and test split\n",
    "splits = df1.randomSplit([0.8, 0.2])\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebd08f79-6deb-42d6-8b45-8f06575b9263",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet = os.environ.get('data_parquet',\n",
    "                              'data.parquet')  # input file name (parquet)\n",
    "master = os.environ.get('master',\n",
    "                        \"local[*]\")  # URL to Spark master\n",
    "model_target = os.environ.get('model_target',\n",
    "                              \"model.xml\")  # model output file name\n",
    "data_dir = os.environ.get('data_dir',\n",
    "                          '../../data/')  # temporary directory for data\n",
    "input_columns = os.environ.get('input_columns',\n",
    "                               '[\"x\", \"y\", \"z\"]')  # input columns to considerv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f1e035c-4943-4d1d-acd2-e2051335ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=eval(input_columns),\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "normalizer = MinMaxScaler(inputCol=\"features\", outputCol=\"features_norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff868c2f-3e7c-4290-9731-c83a8867f339",
   "metadata": {},
   "source": [
    "### Step 5. Train a Random Forest model with different hyperparameters listed below and report the best performing hyperparameter combinations.\n",
    "### Step 6. Use the accuracy metric when evaluating the model with different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b36f2e1-6b49-43db-b726-4efbc1911957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTrees = 10 maxDepth = 5 : Accuracy = 0.44453395994181494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTrees = 10 maxDepth = 7 : Accuracy = 0.4649658722166275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTrees = 20 maxDepth = 5 : Accuracy = 0.44641378538659504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 105:==============>                                          (2 + 6) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTrees = 20 maxDepth = 7 : Accuracy = 0.46922904777889674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#  - number of trees : {10, 20}\n",
    "#  - maximum depth : {5, 7} \n",
    "#  - use random seed = 1 wherever needed\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "for nT in [10, 20]:\n",
    "    for maxD in [5,7]:\n",
    "        # rf = RandomForestClassifier(numTrees = curNTrees, maxDepth = curMaxDepth, seed = 1)\n",
    "        rf = RandomForestClassifier(numTrees=nT, maxDepth=maxD, seed=1)\n",
    "        pipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer, rf])\n",
    "        model = pipeline.fit(df_train)\n",
    "        prediction = model.transform(df_test)\n",
    "        binEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\").setPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "        this_pred = binEval.evaluate(prediction)\n",
    "        print(\"numTrees =\",nT,\"maxDepth =\",maxD,\": Accuracy =\", this_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.911738,
   "end_time": "2021-03-17T17:21:58.127228",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/jovyan/work/examples/pipelines/pairs/component-library/transform/spark-csv-to-parquet.ipynb",
   "output_path": "/home/jovyan/work/examples/pipelines/pairs/component-library/transform/spark-csv-to-parquet.ipynb",
   "parameters": {},
   "start_time": "2021-03-17T17:21:54.215490",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
